{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue:\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median, avg=self.avg, global_avg=self.global_avg, max=self.max, value=self.value\n",
    "        )\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(f\"{name}: {str(meter)}\")\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = \"\"\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
    "        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
    "        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join(\n",
    "                [\n",
    "                    header,\n",
    "                    \"[{0\" + space_fmt + \"}/{1}]\",\n",
    "                    \"eta: {eta}\",\n",
    "                    \"{meters}\",\n",
    "                    \"time: {time}\",\n",
    "                    \"data: {data}\",\n",
    "                    \"max mem: {memory:.0f}\",\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            log_msg = self.delimiter.join(\n",
    "                [header, \"[{0\" + space_fmt + \"}/{1}]\", \"eta: {eta}\", \"{meters}\", \"time: {time}\", \"data: {data}\"]\n",
    "            )\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(\n",
    "                        log_msg.format(\n",
    "                            i,\n",
    "                            len(iterable),\n",
    "                            eta=eta_string,\n",
    "                            meters=str(self),\n",
    "                            time=str(iter_time),\n",
    "                            data=str(data_time),\n",
    "                            memory=torch.cuda.max_memory_allocated() / MB,\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        log_msg.format(\n",
    "                            i, len(iterable), eta=eta_string, meters=str(self), time=str(iter_time), data=str(data_time)\n",
    "                        )\n",
    "                    )\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print(f\"{header} Total time: {total_time_str}\")\n",
    "        \n",
    "def inverse_normalize(img, mean=[0.1307,], std=[0.3081,]):\n",
    "    img = img * torch.tensor(std)\n",
    "    img = img + torch.tensor(mean)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Classification is the most applied task in computer vision and is performed with convolutional neural networks (CNN). The basic idea for classification is to first extract features of an input image an then try to classify those features in order to predict the class of that image.  \n",
    "In this tutorial, we will apply image classification on the MNIST dataset (handwritten numbers).  \n",
    "The first task is to implement the `ConvNet` represented in the figure below.\n",
    "![ConvNet.png](Images/convnet.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(ClassNet, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the data loader\n",
    "We can retrieve the dataset from `PyTorch` and then define our `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mnist = datasets.MNIST(\"/scratch/users/rvandeghen/mnist/\", train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "test_mnist = datasets.MNIST(\"/scratch/users/rvandeghen/mnist/\", train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "                            ]))\n",
    "\n",
    "train_loader = DataLoader(train_mnist, batch_size=32, num_workers=1, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_mnist, batch_size=256, num_workers=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_mnist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF.to_pil_image(TF.resize(inverse_normalize(image), (256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the setup\n",
    "Here we have to define the `ConvNet` model that we will use. We also need to define our loss function and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true):\n",
    "    pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = pred.eq(y_true.view_as(pred)).sum()\n",
    "    accuracy = correct.float()/pred.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = ClassNet(10).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole model\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# feature extractor\n",
    "print(sum(p.numel() for p in model.feature_extractor.parameters() if p.requires_grad))\n",
    "\n",
    "# classifier\n",
    "print(sum(p.numel() for p in model.classifier.parameters() if p.requires_grad))\n",
    "\n",
    "# manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"cv_tuto\",\n",
    "          config={\"batch_size\": 256,\n",
    "                  \"optimizer\": \"sgd\",\n",
    "                  \"lr\": 1e-3,\n",
    "                  \"dataset\": \"mnist\"\n",
    "                 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a training loop\n",
    "We can define a training loop where we pass sequentially the training data and we process the forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_loader, epoch, device, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", SmoothedValue(window_size=1, fmt=\"{value}\"))\n",
    "    metric_logger.add_meter(\"img/s\", SmoothedValue(window_size=10, fmt=\"{value}\"))\n",
    "    \n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(metric_logger.log_every(train_loader, 200, header)):\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        \n",
    "        loss = criterion(outputs, targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "        metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.meters[\"img/s\"].update(batch_size / (time.time() - start_time))\n",
    "        \n",
    "        if (i % 20) == 0:\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "    \n",
    "def test(model, test_loader, device, critetion):\n",
    "    model.eval()\n",
    "    \n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    \n",
    "    header = f\"Test: \"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in metric_logger.log_every(test_loader, 10, header):\n",
    "            outputs = model(inputs.to(device))\n",
    "\n",
    "            loss = criterion(outputs, targets.to(device))\n",
    "\n",
    "            acc = compute_accuracy(outputs, targets.to(device))\n",
    "        \n",
    "            batch_size = inputs.shape[0]\n",
    "            metric_logger.update(loss=loss.item())\n",
    "            metric_logger.meters[\"acc1\"].update(acc.item(), n=batch_size)\n",
    "            \n",
    "    print(f\"{header} Acc@1 {metric_logger.acc1.global_avg:.3f}\")\n",
    "    wandb.log({\"test_acc1\": 100*metric_logger.acc1.global_avg})\n",
    "    \n",
    "def train(num_epoch=10):\n",
    "    for epoch in range(num_epoch):\n",
    "        train_one_epoch(model, optimizer, train_loader, epoch, device, criterion)\n",
    "        test(model, test_loader, device, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
